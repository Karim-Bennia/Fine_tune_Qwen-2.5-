{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T12:50:28.880345Z",
     "iopub.status.busy": "2025-01-30T12:50:28.880058Z",
     "iopub.status.idle": "2025-01-30T12:53:34.620763Z",
     "shell.execute_reply": "2025-01-30T12:53:34.619581Z",
     "shell.execute_reply.started": "2025-01-30T12:50:28.880322Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "!pip install pip3-autoremove\n",
    "!pip-autoremove torch torchvision torchaudio -y\n",
    "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T12:54:35.888302Z",
     "iopub.status.busy": "2025-01-30T12:54:35.888005Z",
     "iopub.status.idle": "2025-01-30T12:54:52.656959Z",
     "shell.execute_reply": "2025-01-30T12:54:52.656293Z",
     "shell.execute_reply.started": "2025-01-30T12:54:35.888279Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.6: Fast Qwen2 patching. Transformers: 4.48.1.\n",
      "   \\\\   /|    GPU: Tesla P100-PCIE-16GB. Max memory: 15.888 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 6.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Sets the maximum sequence length for input data.\n",
    "dtype = None          # Specifies the data type for model weights. Setting to None enables automatic detection based on the hardware. Alternatives include float16 for GPUs like Tesla T4 or V100+\n",
    "load_in_4bit = True  \n",
    "# Enables 4-bit quantization to reduce memory usage, facilitating the loading of larger models without running out of memory (OOM). Setting to False would load the model in higher precision but with higher memory consumption.\n",
    "#unsloth/Qwen2.5-Coder-0.5B\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2-1.5B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T12:54:52.658395Z",
     "iopub.status.busy": "2025-01-30T12:54:52.658088Z",
     "iopub.status.idle": "2025-01-30T12:54:58.860691Z",
     "shell.execute_reply": "2025-01-30T12:54:58.859989Z",
     "shell.execute_reply.started": "2025-01-30T12:54:52.658365Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.1.6 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# configures the model for Parameter-Efficient Fine-Tuning (PEFT) using LoRA (Low-Rank Adaptation) via Unsloth's get_peft_model method\n",
    "# new W=W+ (a*b)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64,  # The rank of the LoRA matrices. Higher values allow more capacity but consume more memory  ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"down_proj\"],  # Specifies which modules within the model to apply LoRA\n",
    "    lora_alpha = 32, # scaling factor for LoRA. Balances the contribution of the LoRA layers to the model's output\n",
    "    lora_dropout = 0.1,  \n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\", # Enables gradient checkpointing to save memory during training. The\n",
    "    random_state = 42,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T12:54:58.862732Z",
     "iopub.status.busy": "2025-01-30T12:54:58.862439Z",
     "iopub.status.idle": "2025-01-30T12:54:58.873406Z",
     "shell.execute_reply": "2025-01-30T12:54:58.872661Z",
     "shell.execute_reply.started": "2025-01-30T12:54:58.862711Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"input\": \"Find ID 3\",\n",
      "  \"expected output\": {\n",
      "    \"type\": \"bracket\",\n",
      "    \"operationCode\": \"&&\",\n",
      "    \"conditions\": [\n",
      "      {\n",
      "        \"type\": \"condition\",\n",
      "        \"operationCode\": \"=\",\n",
      "        \"operand\": \"ID\",\n",
      "        \"value\": 3\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = \"/kaggle/input/datasetgenerated/dataset.json\"\n",
    "\n",
    "with open(dataset_path, \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Print a sample entry\n",
    "print(json.dumps(dataset[0], indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T12:54:58.874729Z",
     "iopub.status.busy": "2025-01-30T12:54:58.874468Z",
     "iopub.status.idle": "2025-01-30T12:54:59.456609Z",
     "shell.execute_reply": "2025-01-30T12:54:59.455793Z",
     "shell.execute_reply.started": "2025-01-30T12:54:58.874709Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Find books published before 2015', 'input': '', 'output': '{\\n  \"type\": \"bracket\",\\n  \"operationCode\": \"&&\",\\n  \"conditions\": [\\n    {\\n      \"type\": \"condition\",\\n      \"operationCode\": \"<\",\\n      \"operand\": \"publication year\",\\n      \"value\": 2015\\n    }\\n  ]\\n}'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert dataset into Alpaca-style format\n",
    "formatted_data = [\n",
    "    {\n",
    "        \"instruction\": entry[\"input\"],\n",
    "        \"input\": \"\",  # Keep empty as instruction is self-contained\n",
    "        \"output\": json.dumps(entry[\"expected output\"], indent=2)\n",
    "    }\n",
    "    for entry in dataset\n",
    "]\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "hf_dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "# Split into train and validation sets\n",
    "hf_dataset = hf_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Show sample formatted data\n",
    "print(hf_dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T12:54:59.457797Z",
     "iopub.status.busy": "2025-01-30T12:54:59.457439Z",
     "iopub.status.idle": "2025-01-30T12:54:59.595270Z",
     "shell.execute_reply": "2025-01-30T12:54:59.594300Z",
     "shell.execute_reply.started": "2025-01-30T12:54:59.457762Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a381ace449674f3e879e5b3dd59f4004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging columns:   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c17c1aefd644f8b169423a35db2849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting to ShareGPT:   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f42af1314d40acac8bd3cdab3f41f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b57ba1e07543f9927bf2a423d6cfa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1401818bc134e92878e3fd3632c9d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4446084f46a44f2899d189e244a7f629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extending conversations:   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'from': 'human', 'value': \"('Find books published before 2015',)\"}, {'from': 'gpt', 'value': '{\\n  \"type\": \"bracket\",\\n  \"operationCode\": \"&&\",\\n  \"conditions\": [\\n    {\\n      \"type\": \"condition\",\\n      \"operationCode\": \"<\",\\n      \"operand\": \"publication year\",\\n      \"value\": 2015\\n    }\\n  ]\\n}'}, {'from': 'human', 'value': \"('Find experiments where (duration less than 48 and protocol type in Standard, Custom) and ((instrument calibration is true and reagent check is true) or supervisor approval not empty)',)\"}, {'from': 'gpt', 'value': '{\\n  \"type\": \"bracket\",\\n  \"operationCode\": \"&&\",\\n  \"conditions\": [\\n    {\\n      \"type\": \"bracket\",\\n      \"operationCode\": \"&&\",\\n      \"conditions\": [\\n        {\\n          \"type\": \"condition\",\\n          \"operationCode\": \"<\",\\n          \"operand\": \"Duration\",\\n          \"value\": 48\\n        },\\n        {\\n          \"type\": \"condition\",\\n          \"operationCode\": \"in\",\\n          \"operand\": \"Protocol Type\",\\n          \"value\": [\\n            \"Standard\",\\n            \"Custom\"\\n          ]\\n        }\\n      ]\\n    },\\n    {\\n      \"type\": \"bracket\",\\n      \"operationCode\": \"||\",\\n      \"conditions\": [\\n        {\\n          \"type\": \"bracket\",\\n          \"operationCode\": \"&&\",\\n          \"conditions\": [\\n            {\\n              \"type\": \"condition\",\\n              \"operationCode\": \"!!\",\\n              \"operand\": \"Instrument Calibration\",\\n              \"value\": true\\n            },\\n            {\\n              \"type\": \"condition\",\\n              \"operationCode\": \"!!\",\\n              \"operand\": \"Reagent Check\",\\n              \"value\": true\\n            }\\n          ]\\n        },\\n        {\\n          \"type\": \"condition\",\\n          \"operationCode\": \"has not\",\\n          \"operand\": \"Supervisor Approval\",\\n          \"value\": null\\n        }\\n      ]\\n    }\\n  ]\\n}'}, {'from': 'human', 'value': \"('Find autonomous vehicles with Level 4 automation',)\"}, {'from': 'gpt', 'value': '{\\n  \"type\": \"bracket\",\\n  \"operationCode\": \"&&\",\\n  \"conditions\": [\\n    {\\n      \"type\": \"condition\",\\n      \"operationCode\": \"=\",\\n      \"operand\": \"automation level\",\\n      \"value\": 4\\n    }\\n  ]\\n}'}]\n"
     ]
    }
   ],
   "source": [
    "from unsloth import to_sharegpt\n",
    "\n",
    "dataset = to_sharegpt(\n",
    "    hf_dataset[\"train\"],\n",
    "    merged_prompt=\"{instruction}\",\n",
    "    output_column_name=\"output\",\n",
    "    conversation_extension=3  # Extend conversations for longer contexts\n",
    ")\n",
    "\n",
    "print(dataset[0][\"conversations\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T12:54:59.596476Z",
     "iopub.status.busy": "2025-01-30T12:54:59.596163Z",
     "iopub.status.idle": "2025-01-30T12:54:59.627464Z",
     "shell.execute_reply": "2025-01-30T12:54:59.626639Z",
     "shell.execute_reply.started": "2025-01-30T12:54:59.596443Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd42daa91db941a19926ef058ef653f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Standardizing format:   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T12:54:59.628739Z",
     "iopub.status.busy": "2025-01-30T12:54:59.628404Z",
     "iopub.status.idle": "2025-01-30T12:54:59.873776Z",
     "shell.execute_reply": "2025-01-30T12:54:59.872956Z",
     "shell.execute_reply.started": "2025-01-30T12:54:59.628708Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: We automatically added an EOS token to stop endless generations.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae73026b607543e3bb5d4cf702955ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{INPUT}\n",
    "\n",
    "### Response:\n",
    "{OUTPUT}\"\"\"\n",
    "\n",
    "from unsloth import apply_chat_template\n",
    "dataset = apply_chat_template(\n",
    "    dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    chat_template=chat_template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T12:54:59.875818Z",
     "iopub.status.busy": "2025-01-30T12:54:59.875608Z",
     "iopub.status.idle": "2025-01-30T12:54:59.902311Z",
     "shell.execute_reply": "2025-01-30T12:54:59.901716Z",
     "shell.execute_reply.started": "2025-01-30T12:54:59.875799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,  # Adjust based on memory\n",
    "    gradient_accumulation_steps=4,  # Reduces GPU memory load\n",
    "    warmup_steps=5,  # Gradual learning rate warm-up\n",
    "    max_steps=50,  # Number of total training steps\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not is_bfloat16_supported(),  # Use FP16 unless BF16 is supported\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",  # Optimizer for low-memory setups\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    "    report_to=\"none\",  # Disable tracking (can use \"wandb\" for logging)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T12:54:59.903351Z",
     "iopub.status.busy": "2025-01-30T12:54:59.903098Z",
     "iopub.status.idle": "2025-01-30T13:00:30.641811Z",
     "shell.execute_reply": "2025-01-30T13:00:30.640958Z",
     "shell.execute_reply.started": "2025-01-30T12:54:59.903318Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8ad98c868c45b6bdf6a83b25d1e5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 152 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 50\n",
      " \"-____-\"     Number of trainable parameters = 36,241,408\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 05:13, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.095600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.110500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.103900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.907100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.930100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.778400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.692400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.647600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.555300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.533500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.525400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.400700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.412700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.390200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.394800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.371900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.382500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.335600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.365600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.266600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.349200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.265500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.337300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.303400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.313500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.324800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.306200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.273200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.299500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.307600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.278700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.290200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.253800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.275300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.261000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.265000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.264100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.277200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.247800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.248200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "trainer_stats = trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T13:07:50.982353Z",
     "iopub.status.busy": "2025-01-30T13:07:50.981957Z",
     "iopub.status.idle": "2025-01-30T13:07:52.047958Z",
     "shell.execute_reply": "2025-01-30T13:07:52.047061Z",
     "shell.execute_reply.started": "2025-01-30T13:07:50.982324Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-qwen2.5-coder/tokenizer_config.json',\n",
       " './fine-tuned-qwen2.5-coder/special_tokens_map.json',\n",
       " './fine-tuned-qwen2.5-coder/vocab.json',\n",
       " './fine-tuned-qwen2.5-coder/merges.txt',\n",
       " './fine-tuned-qwen2.5-coder/added_tokens.json',\n",
       " './fine-tuned-qwen2.5-coder/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./fine-tuned-qwen2.5-coder\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-qwen2.5-coder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T13:07:52.445438Z",
     "iopub.status.busy": "2025-01-30T13:07:52.445101Z",
     "iopub.status.idle": "2025-01-30T13:08:01.791752Z",
     "shell.execute_reply": "2025-01-30T13:08:01.791048Z",
     "shell.execute_reply.started": "2025-01-30T13:07:52.445410Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.6: Fast Qwen2 patching. Transformers: 4.48.1.\n",
      "   \\\\   /|    GPU: Tesla P100-PCIE-16GB. Max memory: 15.888 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 6.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Define the fine-tuned model path\n",
    "fine_tuned_model_path = \"./fine-tuned-qwen2.5-coder\"\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=fine_tuned_model_path,\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T13:08:01.793136Z",
     "iopub.status.busy": "2025-01-30T13:08:01.792903Z",
     "iopub.status.idle": "2025-01-30T13:08:01.797579Z",
     "shell.execute_reply": "2025-01-30T13:08:01.796744Z",
     "shell.execute_reply.started": "2025-01-30T13:08:01.793111Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Move input to GPU if available\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=512,  # Limit output length\n",
    "        temperature=0.1,  # Control randomness (lower = more deterministic)\n",
    "        top_k=50,         # Limit sampling to top-k tokens\n",
    "        top_p=0.9,        # Nucleus sampling\n",
    "        do_sample=True\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T13:08:01.799327Z",
     "iopub.status.busy": "2025-01-30T13:08:01.799066Z",
     "iopub.status.idle": "2025-01-30T13:08:01.823545Z",
     "shell.execute_reply": "2025-01-30T13:08:01.822906Z",
     "shell.execute_reply.started": "2025-01-30T13:08:01.799306Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 1536, padding_idx=151646)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8960, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8960, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T13:08:01.824886Z",
     "iopub.status.busy": "2025-01-30T13:08:01.824612Z",
     "iopub.status.idle": "2025-01-30T13:08:10.404025Z",
     "shell.execute_reply": "2025-01-30T13:08:10.403087Z",
     "shell.execute_reply.started": "2025-01-30T13:08:01.824866Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.6: Fast Qwen2 patching. Transformers: 4.48.1.\n",
      "   \\\\   /|    GPU: Tesla P100-PCIE-16GB. Max memory: 15.888 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 6.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 1536, padding_idx=151646)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8960, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8960, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Define the fine-tuned model path\n",
    "fine_tuned_model_path = \"./fine-tuned-qwen2.5-coder\"\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=fine_tuned_model_path,\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,  # Enables memory-efficient inference\n",
    ")\n",
    "\n",
    "# **Prepare the model for inference** (Required for Unsloth models)\n",
    "FastLanguageModel.for_inference(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T13:08:10.405363Z",
     "iopub.status.busy": "2025-01-30T13:08:10.405013Z",
     "iopub.status.idle": "2025-01-30T13:08:10.409726Z",
     "shell.execute_reply": "2025-01-30T13:08:10.408916Z",
     "shell.execute_reply.started": "2025-01-30T13:08:10.405329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Move input to GPU if available\n",
    "    \n",
    "    # Ensure proper inference settings\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=512,  # Restrict output length\n",
    "        temperature=0.1,  # Control randomness (lower = more deterministic)\n",
    "        top_k=50,         # Limit sampling to top-k tokens\n",
    "        top_p=0.95,        # Nucleus sampling\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T13:08:10.410904Z",
     "iopub.status.busy": "2025-01-30T13:08:10.410629Z",
     "iopub.status.idle": "2025-01-30T13:08:15.369207Z",
     "shell.execute_reply": "2025-01-30T13:08:15.368373Z",
     "shell.execute_reply.started": "2025-01-30T13:08:10.410883Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: Validate barcode XYZ-987654321\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "(\"Barcode XYZ-987654321\",)\n",
      "\n",
      "### Response:\n",
      "\n",
      "{\n",
      "  \"type\": \"bracket\",\n",
      "  \"operationCode\": \"&&\",\n",
      "  \"conditions\": [\n",
      "    {\n",
      "      \"type\": \"condition\",\n",
      "      \"operationCode\": \"=\",\n",
      "      \"operand\": \"Barcode\",\n",
      "      \"value\": \"XYZ-987654321\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_query = \"Validate barcode XYZ-987\"\n",
    "response = generate_response(test_query)\n",
    "print(\"Model Output:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T13:08:15.370381Z",
     "iopub.status.busy": "2025-01-30T13:08:15.370039Z",
     "iopub.status.idle": "2025-01-30T13:08:15.375522Z",
     "shell.execute_reply": "2025-01-30T13:08:15.374607Z",
     "shell.execute_reply.started": "2025-01-30T13:08:15.370347Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_response(user_input):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,  # Required for generation\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=512,  # Limit response length\n",
    "        use_cache=True,\n",
    "        temperature=0.2,  # Reduce randomness for structured output\n",
    "        top_k=40,  # Filter unlikely tokens\n",
    "        top_p=0.95,  # Prevent highly unlikely completions\n",
    "        do_sample=False  # Ensure deterministic JSON output\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T13:08:15.377434Z",
     "iopub.status.busy": "2025-01-30T13:08:15.377223Z",
     "iopub.status.idle": "2025-01-30T13:08:18.320308Z",
     "shell.execute_reply": "2025-01-30T13:08:18.319479Z",
     "shell.execute_reply.started": "2025-01-30T13:08:15.377415Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Validate barcode XYZ-987\n",
      "\n",
      "### Response:\n",
      "{\n",
      "  \"type\": \"bracket\",\n",
      "  \"operationCode\": \"&&\",\n",
      "  \"conditions\": [\n",
      "    {\n",
      "      \"type\": \"condition\",\n",
      "      \"operationCode\": \"=\",\n",
      "      \"operand\": \"barcode\",\n",
      "      \"value\": \"XYZ-987\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_query = \"Validate barcode XYZ-987\"\n",
    "response = generate_response(test_query)\n",
    "print(\"Model Output:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T13:08:18.321564Z",
     "iopub.status.busy": "2025-01-30T13:08:18.321224Z",
     "iopub.status.idle": "2025-01-30T13:08:21.060145Z",
     "shell.execute_reply": "2025-01-30T13:08:21.059393Z",
     "shell.execute_reply.started": "2025-01-30T13:08:18.321526Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Find ID 3\n",
      "\n",
      "### Response:\n",
      "{\n",
      "  \"type\": \"bracket\",\n",
      "  \"operationCode\": \"&&\",\n",
      "  \"conditions\": [\n",
      "    {\n",
      "      \"type\": \"condition\",\n",
      "      \"operationCode\": \"=\",\n",
      "      \"operand\": \"ID\",\n",
      "      \"value\": 3\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_query = \"Find ID 3\"\n",
    "response = generate_response(test_query)\n",
    "print(\"Model Output:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T13:09:01.088792Z",
     "iopub.status.busy": "2025-01-30T13:09:01.088410Z",
     "iopub.status.idle": "2025-01-30T13:09:10.905423Z",
     "shell.execute_reply": "2025-01-30T13:09:10.904629Z",
     "shell.execute_reply.started": "2025-01-30T13:09:01.088764Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Find specimens where (collection date after 2024-01-01 and storage location containing Building A) and ((processing method in Centrifugation, Filtration and volume greater than 50) or analysis notes containing urgent\n",
      "\n",
      "### Response:\n",
      "{\n",
      "  \"type\": \"bracket\",\n",
      "  \"operationCode\": \"&&\",\n",
      "  \"conditions\": [\n",
      "    {\n",
      "      \"type\": \"condition\",\n",
      "      \"operationCode\": \">\",\n",
      "      \"operand\": \"Collection Date\",\n",
      "      \"value\": \"2024-01-01\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"condition\",\n",
      "      \"operationCode\": \"contain\",\n",
      "      \"operand\": \"Storage Location\",\n",
      "      \"value\": \"Building A\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"bracket\",\n",
      "      \"operationCode\": \"||\",\n",
      "      \"conditions\": [\n",
      "        {\n",
      "          \"type\": \"condition\",\n",
      "          \"operationCode\": \"in\",\n",
      "          \"operand\": \"Processing Method\",\n",
      "          \"value\": [\n",
      "            \"Centrifugation\",\n",
      "            \"Filtration\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"type\": \"condition\",\n",
      "          \"operationCode\": \"contain\",\n",
      "          \"operand\": \"Analysis Notes\",\n",
      "          \"value\": \"urgent\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_query = \"Find specimens where (collection date after 2024-01-01 and storage location containing Building A) and ((processing method in Centrifugation, Filtration and volume greater than 50) or analysis notes containing urgent\"\n",
    "response = generate_response(test_query)\n",
    "print(\"Model Output:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6570168,
     "sourceId": 10612581,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
